## Enhancing multi-modal document distillation with energy-weighted supervision[https://www.scopus.com/pages/publications/105017233052?origin=resultslist]

## Review of algorithmic solutions for deployment of neural networks on lite devices[https://crm.ics.org.ru/uploads/crmissues/crm_2024_7/16_khan.pdf]

## Confidence-Based Knowledge Distillation to Reduce Training Costs and Carbon Footprint for Low-Resource Neural Machine Translation[https://www.mdpi.com/2076-3417/15/14/8091]

## Enhancing Energy Efficiency in GAN-based HEVC Video Compression Using Knowledge Distillation[https://ijeces.ferit.hr/index.php/ijeces/article/view/3827/477]

---
# best

## Optimization Strategies for Enhancing Resource Efficiency in Transformers & Large Language Models[https://dl.acm.org/doi/pdf/10.1145/3676151.3719379]

## The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models[https://dl.acm.org/doi/pdf/10.1145/3644815.3644966]

## Mitigating carbon footprint for knowledge distillation based deep learning model compression[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0285668]